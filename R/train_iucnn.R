#' Train an IUCNN Model
#'
#'Trains an IUCNN model based on a data,frame of features,
#'for instance generated by \code{\link{ft_geo}}, \code{\link{ft_clim}},
#'and \code{\link{ft_biom}},
#'and a dataset of labels, (i.e. IUCNN classes) for each species. Note
#'that NAs are not allowed in the features, and taxa with NAs will
#'automatically be removed! Also taxa missing in either the labels
#'or features will be removed.
#'
#'
#'
#'@param x a data.set, containing a column "species"
#'with the species names, and
#'subsequent columns with different features, in the
#'same order as used for \code{\link{predict_iucnn}}.
#'@param lab an object of the class iucnn_labels, as generated by
#' \code{\link{prep_labels}} containing the labels for all species.
#'@param mode character string. Choose between the IUCNN models
#'"nn-class" (default, tensorflow neural network classifier),
#'"nn-reg" (tensorflow neural network regression), or "bnn-class" (Bayesian neural network classifier)
#'@param path_to_output character string. The path to the location
#'where the IUCNN model shall be saved
#'@param validation_split numeric. The fraction of the input data used as validation set.
#'@param test_fraction numeric. The fraction of the input data used as test set.
#'@param seed reset the python random seed.
#'@param max_epochs integer. The maximum number of epochs.
#'@param n_layers numeric vector with length of at least one.
#'The vector quantifies the number of nodes
#'used in each hidden layer of the neural network.
#'This also implicitly specifies the number of hidden
#'layers. For example, n_layers = c(60, 10) defines a model
#'with two hidden layers with 60 and 10 nodes
#' respectively. Note that the number of nodes in the output
#' layer is automatically determined based on
#'the number of unique labels in the training set.
#'@param use_bias logical. Specifies if a bias node is used in the first hidden layer (default=TRUE).
#'@param act_f character string. Specifies the activation function should be used in the hidden layers.
#'Available options are: "relu" (default), "tanh", "sigmoid"
#'@param act_f_out character string. Similar to act_f, this specifies
#'the activation function for the output
#'layer. When setting to "auto" (default), a suitable output activation
#' function will be chosen based on the
#'chosen mode. Other valid options are "softmax" (nn-class, bnn-class), "tanh" (nn-reg),
#'"sigmoid" (nn-reg), or no activation function "" (nn-reg)
#'@param label_stretch_factor numeric. When choosing mode "nn-reg" the
#'labels will be rescaled and this rescaling can be
#'further adjusted by this factor. A factor smaller than 1.0 will
#'compress the range of possible labels.
#'@param randomize_instances logical. When set to TRUE (default) the
#'instances will be shuffled before training (recommended).
#'@param dropout_rate numeric. Apply Monte Carlo dropout to the NN model.
#'This will randomly turn off the specified fraction of
#'nodes of the neural network during each epoch of training
#'as well as during prediction, making the NN more stable and
#'less reliant on individual nodes/weights
#'(only available for modes nn-class and nn-reg).
#'@param label_noise_factor numeric. Add random noise to the input labels after
#' rescaling to give the categorical labels a more
#'continuous spread before training the regression model (only available for mode nn-reg).
#'@param rescale_features logical. Set to TRUE if all feature
#'values shall be rescaled
#'to values between 0 and 1 prior to training (default=FALSE).
#'@param patience integer. Number of epochs with no improvement
#' after which training will be stopped.
#'@param overwrite logical. If TRUE existing models are overwritten. Default is to FALSE,
#'
#'@note See \code{vignette("Approximate_IUCN_Red_List_assessments_with_IUCNN")} for a
#'tutorial on how to run IUCNN.o
#'
#'@return a folder in the working directory (or as specified with path_to_output)
#' with the trained model,
#'for use by \code{\link{predict_iucnn}}.
#'
#' @keywords Prediction

#' @examples
#'\dontrun{
#'data("training_occ") #geographic occurrences of species with IUCN assessment
#'data("training_labels")# the corresponding IUCN assessments
#'data("prediction_occ") #occurrences from Not Evaluated species to prdict
#'
#'# 1. Feature and label preparation
#'features <- prep_features(training_occ) # Training features
#'labels_train <- prep_labels(training_labels) # Training labels
#'features_predict <- prep_features(prediction_occ) # Prediction features
#'
#'# 2. Model training
#'m1 <- train_iucnn(x = features, lab = labels_train)
#'
#'summary(m1)
#'plot(m1)
#'}
#'
#'
#' @export
#' @importFrom reticulate py_get_attr source_python
#' @importFrom stats complete.cases
#' @importFrom checkmate assert_data_frame assert_character assert_logical assert_numeric

train_iucnn <- function(x,
                        lab,
                        path_to_output = "iuc_nn_model",
                        best_model = FALSE,
                        mode = 'nn-class',
                        validation_fraction = 0.1,
                        cv_fold = 1,
                        seed = 1234,
                        max_epochs = 1000,
                        patience = 200,
                        n_layers = '60_60_20',
                        use_bias = TRUE,
                        act_f = "relu",
                        act_f_out = "auto",
                        label_stretch_factor = 1.0,
                        randomize_instances = TRUE,
                        dropout_rate = 0.0,
                        mc_dropout = TRUE,
                        mc_dropout_reps = 100,
                        label_noise_factor = 0.0,
                        rescale_features = FALSE,
                        save_model = TRUE,
                        overwrite = FALSE,
                        verbose = 1){

  # Check input
  ## assertion
  assert_data_frame(x)
  assert_class(lab, classes = "iucnn_labels")
  assert_character(path_to_output)
  assert_numeric(validation_fraction, lower = 0, upper = 1)
  assert_numeric(seed)
  assert_numeric(max_epochs)
  assert_character(n_layers)
  assert_logical(use_bias)
  assert_character(act_f)
  assert_character(act_f_out)
  assert_numeric(label_stretch_factor, lower = 0, upper = 2)
  assert_numeric(patience)
  assert_logical(randomize_instances)
  assert_numeric(dropout_rate, lower = 0, upper = 1)
  assert_numeric(label_noise_factor, lower = 0, upper = 1)
  assert_logical(rescale_features)
  assert_logical(overwrite)
  match.arg(mode, choices = c("nn-class", "nn-reg", "bnn-class"))

  if (class(best_model)=="data.frame"){
    #warning("Model settings are being adopted from info provided under 'best_model' flag. All other provided model settings are being ignored. validation_fraction is set to 0 and cv_fold to 1.")
    mode = best_model$mode
    dropout_rate = best_model$dropout_rate
    seed = best_model$seed
    max_epochs = best_model$final_train_epoch_all
    max_epochs = round(mean(as.numeric(strsplit(max_epochs,'_')[[1]])))
    patience = NULL
    n_layers = best_model$n_layers
    use_bias = best_model$use_bias
    rescale_features = best_model$rescale_features
    randomize_instances = best_model$randomize_instances
    mc_dropout = best_model$mc_dropout
    mc_dropout_reps = 100
    act_f = best_model$act_f
    act_f_out = best_model$act_f_out
    cv_fold = 1
    validation_fraction = 0.
    label_stretch_factor = best_model$label_stretch_factor
    label_noise_factor = best_model$label_noise_factor
    overwrite = TRUE
  }

  # check if the model directory already exists
  if(dir.exists(file.path(path_to_output))& !overwrite){
    stop(sprintf("Directory %s exists. Provide alternative 'path_to_output' or set `overwrite` to TRUE.", path_to_output))
  }

  data_out = process_iucnn_input(x,lab = lab, mode = mode, outpath = '.', write_data_files = FALSE, verbose=verbose)

  dataset = data_out[[1]]
  labels = data_out[[2]]
  instance_names = data_out[[3]]

  n_layers = as.numeric(strsplit(n_layers,'_')[[1]])

  # set out act fun if chosen auto
  if (act_f_out == 'auto'){
    if (mode == 'nn-reg'){
      act_f_out  <-  'tanh'
    }else{
      act_f_out  <-  'softmax'
    }
  }

  if (dropout_rate > 0.0){
    dropout_boolean <- TRUE
  }else{
    dropout_boolean <- FALSE
  }

  if (mode == 'bnn-class'){
    act_f = 'swish'
    print('Activation function for BNN model is hard-coded to be "swish" and is not affected by the act_f setting.')
    # transform the data into BNN compatible format
    bnn_data <- bnn_load_data(dataset,
                             labels,
                             seed = as.integer(seed),
                             testsize = validation_fraction,
                             all_class_in_testset=FALSE,
                             header = TRUE, # input data has a header
                             instance_id = TRUE, # input data includes names of instances
                             from_file = FALSE
    )

    # define number of layers and nodes per layer for BNN
    # define the BNN model
    bnn_model <- create_BNN_model(bnn_data,
                                 n_layers,
                                 actfun = act_f,
                                 seed = as.integer(seed)
    )

    # set up the MCMC environment
    update_frequencies <- rep(0.05, length(n_layers) + 1)
    update_window_sizes <- rep(0.075, length(n_layers) + 1)
    mcmc_object <- MCMC_setup(bnn_model,
                             update_frequencies,
                             update_window_sizes,
                             n_iteration = as.integer(max_epochs),
                             sampling_f = 10
    )

    # run the MCMC and write output to file
    logger <- run_MCMC(bnn_model,
                      mcmc_object,
                      filename_stem = paste0(path_to_output,'/','BNN')
    )

    # calculate test accuracy
    post_pr_test <- calculate_accuracy(bnn_data,
                                       logger,
                                       bnn_model,
                                       post_summary_mode = 0
    )

    input_data <- bnn_data

    logfile_path <- as.character(py_get_attr(logger, '_logfile'))
    log_file_content <- read.table(logfile_path, sep = '\t', header = TRUE)
    pklfile_path <- as.character(py_get_attr(logger, '_pklfile'))

    validation_labels <- bnn_data$test_labels
    validation_predictions <- apply(post_pr_test$post_prob_predictions, 1, which.max) - 1
    validation_predictions_raw <- post_pr_test$post_prob_predictions

    confusion_matrix <- post_pr_test$confusion_matrix
    confusion_matrix <- confusion_matrix[1:dim(confusion_matrix)[1] - 1, 1:dim(confusion_matrix)[2] - 1] #remove the sum row and column

    training_accuracy <- log_file_content$accuracy[length(log_file_content$accuracy)]
    validation_accuracy <- post_pr_test$mean_accuracy

    training_loss <- (-log_file_content$likelihood[length(log_file_content$likelihood)]) / length(bnn_data$labels)
    validation_loss <- NaN

    training_loss_history <- (-log_file_content$likelihood) / length(bnn_data$labels)
    validation_loss_history <- NaN

    training_accuracy_history <- log_file_content$accuracy
    validation_accuracy_history <- NaN

    training_mae_history <- NaN
    validation_mae_history <- NaN

    rescale_labels_boolean <- FALSE
    label_rescaling_factor <- as.integer(max(labels$labels))
    min_max_label <- as.vector(c(min(labels$labels), max(labels$labels)))
    label_stretch_factor <- label_stretch_factor

    activation_function <- act_f_out
    trained_model_path <- pklfile_path
    patience = NaN
    validation_fraction = validation_fraction

    accthres_tbl = NaN
    stopping_point = NaN

  }else{

    # source python function
    reticulate::source_python(system.file("python", "IUCNN_train.py", package = "IUCNN"))

    # run model via python script
    res <- iucnn_train(dataset = as.matrix(dataset),
                      labels = as.matrix(labels),
                      mode = mode,
                      path_to_output = path_to_output,
                      validation_fraction = validation_fraction,
                      cv_k = as.integer(cv_fold),
                      seed = as.integer(seed),
                      instance_names = as.matrix(instance_names),
                      feature_names = names(dataset),
                      verbose = 0,
                      max_epochs = as.integer(max_epochs),
                      patience = patience,
                      n_layers = as.list(n_layers),
                      use_bias = use_bias,
                      act_f = act_f,
                      act_f_out = act_f_out,
                      stretch_factor_rescaled_labels = label_stretch_factor,
                      randomize_instances = as.integer(randomize_instances),
                      rescale_features = rescale_features,
                      dropout_rate = dropout_rate,
                      dropout_reps = mc_dropout_reps,
                      mc_dropout = mc_dropout,
                      label_noise_factor = label_noise_factor,
                      save_model = save_model
    )

    validation_labels <- as.vector(res[[1]])
    validation_predictions <- as.vector(res[[2]])
    validation_predictions_raw <- res[[3]]

    training_accuracy <- res[[4]]
    validation_accuracy <- res[[5]]

    training_loss <- res[[6]]
    validation_loss <- res[[7]]

    training_loss_history <- res[[8]]
    validation_loss_history <- res[[9]]

    training_accuracy_history <- res[[10]]
    validation_accuracy_history <- res[[11]]

    training_mae_history <- res[[12]]
    validation_mae_history <- res[[13]]

    rescale_labels_boolean <- res[[14]]
    label_rescaling_factor <- res[[15]]
    min_max_label <- as.vector(res[[16]])
    label_stretch_factor <- res[[17]]

    activation_function <- res[[18]]
    trained_model_path <- res[[19]]

    confusion_matrix <- res[[20]]
    accthres_tbl <- res[[21]]
    stopping_point <- res[[22]]

    input_data <- res[[23]]
    }

  named_res <- NULL

  named_res$input_data <- c(input_data, lookup = data.frame(lab$lookup))

  named_res$rescale_labels_boolean <- rescale_labels_boolean
  named_res$label_rescaling_factor <- label_rescaling_factor
  named_res$min_max_label_rescaled <- min_max_label
  named_res$label_stretch_factor <- label_stretch_factor

  named_res$trained_model_path <- trained_model_path
  named_res$accthres_tbl <- accthres_tbl
  named_res$final_training_epoch <- stopping_point

  named_res$model <- mode
  named_res$seed <- seed
  named_res$dropout <- dropout_boolean
  named_res$dropout_rate <- dropout_rate
  named_res$max_epochs <- max_epochs
  named_res$n_layers <- n_layers
  named_res$use_bias <- use_bias
  named_res$rescale_features <- rescale_features
  named_res$act_f <- act_f
  named_res$act_f_out <- activation_function
  named_res$validation_fraction <- validation_fraction
  named_res$cv_fold <- cv_fold
  named_res$patience <- patience
  named_res$randomize_instances <- randomize_instances
  named_res$label_noise_factor <- label_noise_factor
  named_res$mc_dropout <- mc_dropout
  named_res$mc_dropout_reps <- mc_dropout_reps

  named_res$training_loss_history <- training_loss_history
  named_res$validation_loss_history <- validation_loss_history

  named_res$training_accuracy_history <- training_accuracy_history
  named_res$validation_accuracy_history <- validation_accuracy_history

  named_res$training_mae_history <- training_mae_history
  named_res$validation_mae_history <- validation_mae_history

  named_res$training_loss <- training_loss
  named_res$validation_loss <- validation_loss

  named_res$validation_predictions_raw <- validation_predictions_raw #softmax probs, posterior probs, or regressed values
  named_res$validation_predictions <- validation_predictions
  named_res$validation_labels <- validation_labels

  named_res$confusion_matrix <- confusion_matrix

  named_res$training_accuracy <- training_accuracy
  named_res$validation_accuracy <- validation_accuracy

  class(named_res) <- "iucnn_model"

  return(named_res)
}
