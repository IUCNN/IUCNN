% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modeltest_iucnn.R
\name{modeltest_iucnn}
\alias{modeltest_iucnn}
\title{Model-testing IUCNN models using cross-validation (hyperparameter-tuning)}
\usage{
modeltest_iucnn(
  x,
  lab,
  logfile = "model_testing_logfile.txt",
  model_outpath = "modeltest",
  mode = "nn-class",
  cv_fold = 5,
  validation_fraction = 0,
  n_layers = c("50_30_10", "30"),
  dropout_rate = c(0, 0.1, 0.3),
  use_bias = TRUE,
  balance_classes = FALSE,
  seed = 1234,
  label_stretch_factor = 1,
  label_noise_factor = 0,
  act_f = "relu",
  act_f_out = "auto",
  max_epochs = 5000,
  patience = 200,
  mc_dropout = TRUE,
  mc_dropout_reps = 100,
  randomize_instances = TRUE,
  rescale_features = FALSE,
  init_logfile = TRUE,
  recycle_settings = FALSE
)
}
\arguments{
\item{x}{a data.set, containing a column "species"
with the species names, and
subsequent columns with different features, in the
same order as used for \code{\link{predict_iucnn}}.}

\item{lab}{an object of the class iucnn_labels, as generated by
\code{\link{prep_labels}} containing the labels for all species.}

\item{logfile}{a string with the filepath/name for the output log-file.}

\item{mode}{character string. Choose between the IUCNN models
"nn-class" (default, tensorflow neural network classifier),
"nn-reg" (tensorflow neural network regression), or
"bnn-class" (Bayesian neural network classifier)}

\item{n_layers}{numeric vector with length of at least one.
The vector quantifies the number of nodes
used in each hidden layer of the neural network.
This also implicitly specifies the number of hidden
layers. For example, n_layers = c(60, 10) defines a model
with two hidden layers with 60 and 10 nodes
respectively. Note that the number of nodes in the output
layer is automatically determined based on
the number of unique labels in the training set.}

\item{dropout_rate}{numeric. Apply Monte Carlo dropout to the NN model.
This will randomly turn off the specified fraction of
nodes of the neural network during each epoch of training
as well as during prediction, making the NN more stable and
less reliant on individual nodes/weights
(only available for modes nn-class and nn-reg).}

\item{use_bias}{logical. Specifies if a bias node is used in
the first hidden layer (default=TRUE).}

\item{seed}{reset the python random seed.}

\item{label_stretch_factor}{numeric. When choosing mode "nn-reg" the
labels will be rescaled and this rescaling can be
further adjusted by this factor. A factor smaller than 1.0 will
compress the range of possible labels.}

\item{label_noise_factor}{numeric. Add random noise to the input labels after
rescaling to give the categorical labels a more
continuous spread before training the regression model
(only available for mode nn-reg).}

\item{act_f}{character string. Specifies the activation
function should be used in the hidden layers.
Available options are: "relu" (default), "tanh", "sigmoid"}

\item{act_f_out}{character string. Similar to act_f, this specifies
the activation function for the output
layer. When setting to "auto" (default), a suitable output activation
function will be chosen based on the
chosen mode. Other valid options are "softmax"
(nn-class, bnn-class), "tanh" (nn-reg),
"sigmoid" (nn-reg), or no activation function "" (nn-reg)}

\item{max_epochs}{integer. The maximum number of epochs.}

\item{patience}{integer. Number of epochs with no improvement
after which training will be stopped.}

\item{randomize_instances}{logical. When set to TRUE (default) the
instances will be shuffled before training (recommended).}

\item{rescale_features}{logical. Set to TRUE if all feature
values shall be rescaled
to values between 0 and 1 prior to training (default=FALSE).}

\item{init_logfile}{logical (default=TRUE). If set to TRUE,
\code{modeltest_iucnn} will attempt to initiate a new log-file under the
provided path, possibly overwriting already existing model-testing results
stored in the same location. Set to FALSE if instead you want to append to
an already existing log-file.}

\item{recycle_settings}{logical (default=FALSE). If set to TRUE,
\code{modeltest_iucnn} will read the log-file stored at the path specified
under the "logfile" argument and run model-testing for the input features
and labels using the same models stored in that file. This setting can be
useful when e.g. wanting to test the same models for different sets of input
data.}
}
\value{
outputs a data.frame object containing stats and settings of all
tested models.
}
\description{
Takes as input features produced with \code{\link{prep_features}}
and labels produced with \code{\link{prep_labels}}, as well as a path to a
log-file where results for each tested model will be stored. All available
options are identical to the \code{\link{train_iucnn}} function and can be
provided as vectors, e.g. \code{dropout_rate = c(0.0,0.1,0.3)} and
\code{n_layers = c('30','40_20','50_30_10')}. \code{modeltest_iucnn} will
then iterate through all possible permutations of the provided hyperparameter
settings, train a separate model for each hyperparameter combination, and
store the results in the provided log-file.
}
\note{
See \code{vignette("Approximate_IUCN_Red_List_assessments_with_IUCNN")} for a
tutorial on how to run IUCNN.
}
\examples{
\dontrun{
# Model-testing
logfile = paste0("model_testing_results.txt")
model_testing_results = modeltest_iucnn(features,
                                        labels,
                                        logfile,
                                        model_outpath = 'iucnn_modeltest',
                                        mode = 'nn-class',
                                        seed = 1234,
                                        dropout_rate = c(0.0,0.1,0.3),
                                        n_layers = c('30','40_20','50_30_10'),
                                        cv_fold = 5,
                                        init_logfile = TRUE)
}


}
