% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iucnn_train_model.R
\name{iucnn_train_model}
\alias{iucnn_train_model}
\title{Train an IUCNN Model}
\usage{
iucnn_train_model(
  x,
  lab,
  path_to_output = tempdir(),
  production_model = NULL,
  mode = "nn-class",
  test_fraction = 0.2,
  cv_fold = 1,
  seed = 1234,
  max_epochs = 1000,
  patience = 200,
  batch_size = 32,
  n_layers = "50_30_10",
  use_bias = TRUE,
  balance_classes = FALSE,
  act_f = "auto",
  act_f_out = "auto",
  label_stretch_factor = 1,
  randomize_instances = TRUE,
  dropout_rate = 0,
  mc_dropout = TRUE,
  mc_dropout_reps = 100,
  label_noise_factor = 0,
  rescale_features = FALSE,
  save_model = TRUE,
  overwrite = FALSE,
  optimizer = "adam",
  optimizer_args = NULL,
  l2_regularizer = NULL,
  verbose = 1
)
}
\arguments{
\item{x}{a data.set, containing a column "species"
with the species names, and
subsequent columns with different features.}

\item{lab}{an object of the class iucnn_labels, as generated by
\code{\link{iucnn_prepare_labels}} containing the labels for all species.}

\item{path_to_output}{character string. The path to the location
where the IUCNN model shall be saved}

\item{production_model}{an object of type iucnn_model (default=NULL).
If an iucnn_model is provided, \code{iucnn_train_model} will read the settings of
this model and reproduce it, but use all available data for training, by
automatically setting the validation set to 0 and cv_fold to 1. This is
recommended before using the model for predicting the IUCN status of
not evaluated species, as it generally improves the prediction
accuracy of the model. Choosing this option will ignore all other provided
settings (below).}

\item{mode}{character string. Choose between the IUCNN models
"nn-class" (default, tensorflow neural network classifier),
"nn-reg" (tensorflow neural network regression), or
"bnn-class" (Bayesian neural network classifier)}

\item{test_fraction}{numeric. The fraction of the input data used as
test set.}

\item{cv_fold}{integer (default=1). When setting cv_fold > 1,
\code{iucnn_train_model} will perform k-fold cross-validation. In this case, the
provided setting for test_fraction will be ignored, as the test
size of each CV-fold is determined by the specified number provided here.}

\item{seed}{integer. Set a starting seed for reproducibility.}

\item{max_epochs}{integer. The maximum number of epochs.}

\item{patience}{integer. Number of epochs with no improvement
after which training will be stopped.}

\item{batch_size}{integer. Number of samples per gradient update.
If unspecified, batch_size will default to 32.}

\item{n_layers}{character string. Define number node per layer by providing a
character string where the number of nodes for each layer are separated by
underscores. E.g. '50_30_10' (default) will train a model with 3 hidden layers with
50, 30, and 10 nodes respectively. Note that the number of nodes in the output
layer is automatically determined based on
the number of unique labels in the training set.}

\item{use_bias}{logical (default=TRUE). Specifies if a bias node is used in
the first hidden layer.}

\item{balance_classes}{logical (default=FALSE). If set to TRUE,
\code{iucnn_train_model} will perform supersampling of the training instances to
account for uneven class distribution in the training data. In case of
training an bnn-class model, choosing this option will add the estimation
of class weights instead, to account for class imbalances.}

\item{act_f}{character string. Specifies the activation
function should be used in the hidden layers.
Available options are: "relu", "tanh", "sigmoid", or "swish" (latter only for
bnn-class). If set to 'auto' (default), \code{iucnn_train_model} will pick a reasonable
default ('relu' for nn-class or nn-reg, and 'swish' for bnn-class).}

\item{act_f_out}{character string. Similar to act_f, this specifies
the activation function for the output
layer. Available options are "softmax" (nn-class, bnn-class), "tanh" (nn-reg),
"sigmoid" (nn-reg), or no activation function "" (nn-reg). When set to "auto"
(default), a suitable output activation function will be chosen based on the
chosen mode ('softmax' for nn-class or bnn-class, 'tanh' for nn-reg).}

\item{label_stretch_factor}{numeric (only for mode nn-reg). The provided
value will be applied as a factor to stretch or compress the labels before
training a regression model. A factor smaller < 1.0 will compress the range
of labels, while a factor > 1 will stretch the range.}

\item{randomize_instances}{logical. When set to TRUE (default) the
instances will be shuffled before training (recommended).}

\item{dropout_rate}{numeric. This will randomly turn off the specified
fraction of nodes of the neural network during each epoch of training
making the NN more stable and less reliant on individual nodes/weights, which
can prevent over-fitting (only available for modes nn-class and nn-reg).
See mc_dropout setting explained below if dropout shall also be applied to the
predictions.}

\item{mc_dropout}{logical. If set to TRUE, the predictions (including the
validation accuracy) based on a model trained with a dropout fraction > 0
will reflect the stochasticity introduced by the dropout method (MC dropout
predictions). This is e.g. required when wanting to predict with a specified
accuracy threshold (see target_acc option in \code{\link{iucnn_predict_status}}).
This option is activated by default when chosing a dropout_rate > 0, unless
it is manually set to FALSE here.}

\item{mc_dropout_reps}{integer. The number of MC iterations to run when
predicting validation accuracy and calculating the accuracy-threshold
table required for making predictions with an accuracy threshold.
The default of 100 is usually sufficient, larger values will lead to longer
computation times, particularly during model testing with cross-validation.}

\item{label_noise_factor}{numeric (only for mode nn-reg). Add specified amount
of random noise to the input labels to give the categorical labels a more
continuous spread before training the regression model. E.g. a value of 0.2
will redraw a label of a species categorized as Vulnerable (class=2) randomly
between 1.8 and 2.2, based on a uniform probability distribution.}

\item{rescale_features}{logical. Set to TRUE if all feature values shall
be rescaled to values between 0 and 1 prior to training (default=FALSE).}

\item{save_model}{logical. If TRUE the model is saved to disk.}

\item{overwrite}{logical. If TRUE existing models are
overwritten. Default is set to FALSE.}

\item{optimizer}{character string. Default adam. Sets the tensorflow
optimizer.}

\item{optimizer_args}{named list. Default NULL.
Provides optional arguments for the tensorflow optimizers. See tensorflow
documentation.}

\item{l2_regularizer}{named list. Default NULL.
Provides optional values for L2 regularization for the kernel_regularizer,
bias_regularizer, and activity_regularizer. E.g list(kernel_regularizer = 1e-4)}

\item{verbose}{Default 0, set to 1 for \code{iucnn_train_model} to print
additional info to the screen while training.}
}
\value{
outputs an \code{iucnn_model} object which can be used in
\code{\link{iucnn_predict_status}} for predicting the conservation status
of not evaluated species.
}
\description{
Trains an IUCNN model based on a data.frame of features for a set of species,
generated by \code{\link{iucnn_prepare_features}},
and the corresponding IUCN classes formatted as a iucnn_labels object
with \code{\link{iucnn_prepare_labels}}. Note
that NAs are not allowed in the features, and taxa with NAs will
automatically be removed! Taxa, for which information is only present in one
of the two input objects will be removed as well.
}
\note{
See \code{vignette("Approximate_IUCN_Red_List_assessments_with_IUCNN")}
for a tutorial on how to run IUCNN.
}
\examples{
\dontrun{
data("training_occ") #geographic occurrences of species with IUCN assessment
data("training_labels")# the corresponding IUCN assessments

# 1. Feature and label preparation
features <- iucnn_prepare_features(training_occ, type = "geographic") # Training features
labels_train <- iucnn_prepare_labels(training_labels, features) # Training labels

# 2. Model training
m1 <- iucnn_train_model(x = features, lab = labels_train, overwrite = TRUE)

summary(m1)
plot(m1)

# 3. Change optimizer
m2 <- iucnn_train_model(x = features, lab = labels_train, overwrite = TRUE,
                       optimizer = "SGD", optimizer_args = list(learning_rate = 0.02))
summary(m2)
}


}
\keyword{Training}
